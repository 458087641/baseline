{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# COMP 7310 Personal Project","metadata":{}},{"cell_type":"markdown","source":"### 1. Config Setup","metadata":{}},{"cell_type":"code","source":"# Make Dirs for Model Storage and Test Results\nimport os\n\ntry:\n    # Model Storage Path\n    model_prediction_path = '/kaggle/working/prediction_model'\n    os.mkdir(model_prediction_path)\nexcept:\n    pass\ntry:\n    # Test Results Path\n    test_result_path = '/kaggle/working/test_results'\n    os.mkdir(test_result_path)\nexcept:\n    pass\ntry:\n    # Test Seen & Unseen Path\n    os.mkdir(test_result_path + '/test_seen')\n    os.mkdir(test_result_path + '/test_unseen')\nexcept:\n    pass","metadata":{"execution":{"iopub.status.busy":"2023-10-10T11:42:35.653280Z","iopub.execute_input":"2023-10-10T11:42:35.653762Z","iopub.status.idle":"2023-10-10T11:42:35.663389Z","shell.execute_reply.started":"2023-10-10T11:42:35.653738Z","shell.execute_reply":"2023-10-10T11:42:35.662462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nconfig settings:\n\"\"\"\n### TFNET parameters\nBATCH_SIZE = 72 # Training batch size\nTEST_BATCH_SIZE = 1 # Test batch size\nEPOCHS = 150 # Traning epoch\nSAVE_INTERVAL = 20 # Save model every 20 epochs\nSTEP_SIZE = 200 # Step size for moving forward the window (For training)\nTEST_STEP_SIZE = 400 # Step size for moving forward the window (For testing)\nWINDOW_SIZE = 400 # Window size for training and testing\nINPUT_CHANNEL = 6 # Input feature dimension (Gryo + Acce)\nOUTPUT_CHANNEL = 2 # Output dimension (2D velocity vector)\nSAMPLING_RATE = 200 # Sampling rate\nLAYER_SIZE = 100 # The size of LSTM\nLAYERS = 3 # The layer size of LSTM\nDROPOUT = 0.1 # Dropout probability\nLEARNING_RATE = 0.0003 # Learning rate\nNUM_WORKERS = 8\n### ------------------ ###\n\n### Data preprocessing parameters\nFEATURE_SIGMA = 2.0 # Sigma for feature gaussian smoothing\nTARGET_SIGMA = 30.0 # Sigma for target gaussian smoothing\n### ------------------ ###\n\n### Device for training\nDEVICE = \"cuda:0\" # You can choose GPU or CPU\n### ------------------ ###\n\n### Training and testing setting\nDATA_DIR = '/kaggle/input/comp7310-project-1-imu-indoor-tracking/original_data/train_dataset' # Dataset directory for training\nVAL_DATA_DIR = '/kaggle/input/comp7310-project-1-imu-indoor-tracking/original_data/val_dataset' # Dataset directory for validation\nTEST_DIR = '/kaggle/input/comp7310-project-1-imu-indoor-tracking/original_data/test_seen' # Dataset directory for testing (unseen_subjects_test_set)\nOUT_DIR = '/kaggle/working/prediction_model' # Output directory for both traning and testing\nMODEL_PATH = '' # Model path for testing\n### ------------------ ###\n\ndef load_config():\n    kwargs = {}\n    kwargs['batch_size'] = BATCH_SIZE\n    kwargs['test_batch_size'] = TEST_BATCH_SIZE\n    kwargs['epochs'] = EPOCHS\n    kwargs['save_interval'] = SAVE_INTERVAL\n    kwargs['step_size'] = STEP_SIZE\n    kwargs['test_step_size'] = TEST_STEP_SIZE\n    kwargs['window_size'] = WINDOW_SIZE\n    kwargs['sampling_rate'] = SAMPLING_RATE\n    kwargs['input_channel'] = INPUT_CHANNEL\n    kwargs['output_channel'] = OUTPUT_CHANNEL\n    kwargs['layer_size'] = LAYER_SIZE\n    kwargs['layers'] = LAYERS\n    kwargs['dropout'] = DROPOUT\n    kwargs['learning_rate'] = LEARNING_RATE\n    kwargs['num_workers'] = NUM_WORKERS\n\n    kwargs['feature_sigma'] = FEATURE_SIGMA\n    kwargs['target_sigma'] = TARGET_SIGMA\n\n    kwargs['device'] = DEVICE\n\n    kwargs['data_dir'] = DATA_DIR\n    kwargs['val_data_dir'] = VAL_DATA_DIR\n    kwargs['test_dir'] = TEST_DIR\n    kwargs['out_dir'] = OUT_DIR\n    kwargs['model_path'] = MODEL_PATH\n\n    return kwargs\n","metadata":{"execution":{"iopub.status.busy":"2023-10-10T11:42:39.769453Z","iopub.execute_input":"2023-10-10T11:42:39.769860Z","iopub.status.idle":"2023-10-10T11:42:39.784639Z","shell.execute_reply.started":"2023-10-10T11:42:39.769795Z","shell.execute_reply":"2023-10-10T11:42:39.783454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Model Design","metadata":{}},{"cell_type":"code","source":"import torch\nfrom scipy import signal\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torchvision.models as models\nfrom torch.nn.utils import weight_norm\n\n### BiLSTM model\nclass BilinearLSTMSeqNetwork(torch.nn.Module):\n    def __init__(self, input_size, out_size, batch_size, device,\n                 lstm_size=100, lstm_layers=3, dropout=0):\n        \"\"\"\n        LSTM network with Bilinear layer\n        Input: torch array [batch x frames x input_size]\n        Output: torch array [batch x frames x out_size]\n        :param input_size: num. channels in input\n        :param out_size: num. channels in output\n        :param batch_size:\n        :param device: torch device\n        :param lstm_size: number of LSTM units per layer\n        :param lstm_layers: number of LSTM layers\n        :param dropout: dropout probability of LSTM (@ref https://pytorch.org/docs/stable/nn.html#lstm)\n        \"\"\"\n        super(BilinearLSTMSeqNetwork, self).__init__()\n        self.input_size = input_size\n        self.lstm_size = lstm_size\n        self.output_size = out_size\n        self.num_layers = lstm_layers\n        self.batch_size = batch_size\n        self.device = device\n\n        self.bilinear = torch.nn.Bilinear(self.input_size, self.input_size, self.input_size * 4)\n        self.lstm = torch.nn.LSTM(self.input_size * 5, self.lstm_size, self.num_layers, batch_first=True, dropout=dropout)\n        self.linear1 = torch.nn.Linear(self.lstm_size + self.input_size * 5, self.output_size * 5)\n        self.linear2 = torch.nn.Linear(self.output_size * 5, self.output_size)\n        self.hidden = self.init_weights()\n\n    def forward(self, input):\n        input_mix = self.bilinear(input, input)\n        input_mix = torch.cat([input, input_mix], dim=2)\n        output, self.hidden = self.lstm(input_mix, self.init_weights())\n        output = torch.cat([input_mix, output], dim=2)\n        output = self.linear1(output)\n        output = self.linear2(output)\n        return output\n\n    def init_weights(self):\n        h0 = torch.zeros(self.num_layers, self.batch_size, self.lstm_size)\n        c0 = torch.zeros(self.num_layers, self.batch_size, self.lstm_size)\n        h0 = h0.to(self.device)\n        c0 = c0.to(self.device)\n        return Variable(h0), Variable(c0)\n### --------------------- End of the model --------------------- ###","metadata":{"execution":{"iopub.status.busy":"2023-10-10T11:42:40.813759Z","iopub.execute_input":"2023-10-10T11:42:40.814135Z","iopub.status.idle":"2023-10-10T11:42:47.117410Z","shell.execute_reply.started":"2023-10-10T11:42:40.814105Z","shell.execute_reply":"2023-10-10T11:42:47.116424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Data Loader","metadata":{}},{"cell_type":"code","source":"!pip install pyquaternion==0.9.9\n!pip install numpy-quaternion==2022.4.3","metadata":{"execution":{"iopub.status.busy":"2023-10-10T11:42:47.118998Z","iopub.execute_input":"2023-10-10T11:42:47.119972Z","iopub.status.idle":"2023-10-10T11:43:07.457377Z","shell.execute_reply.started":"2023-10-10T11:42:47.119939Z","shell.execute_reply":"2023-10-10T11:43:07.456059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport random\nimport os\nfrom os import path as osp\n\nimport h5py\nimport torch\nimport numpy as np\nimport quaternion\nimport math\nfrom scipy.ndimage import gaussian_filter1d\nfrom torch.utils.data import Dataset\n\ndef convert_data(data_path):\n    \"\"\"\n    Data Processing\n    :param data_path\n    This function is used to convert the raw data\n    stored in hdf5 format to numpy array\n    \"\"\"\n    # read hdf5 file\n    file = h5py.File(os.path.join(data_path, 'data.hdf5'), 'r')\n    synced = file['synced'] # synced data (gyro, acce, magn ...)\n    pose = file['pose'] # pose data (ground truth)\n    # timestamp (1D)\n    timestamps = np.array(synced['time'])\n    timestamps = timestamps.reshape((len(timestamps), 1)) / 10**9 # - timestamps[0] # start from 0\n    timestamps = timestamps - timestamps[0]\n    # gyroscope (3D), accelerometer (3D), magnetometer (3D), rotation vector (4D)\n    gyro = np.array(synced['gyro'])\n    acce = np.array(synced['acce'])\n    # game rotation vector can convert acce and gyro\n    # from body frame to navigation frame\n    rotation_vector = synced['game_rv']\n\n    # position (3D), orientation (4D)\n    pose = file['pose']\n    tango_pos = np.array(pose['tango_pos'])\n    tango_ori = np.array(pose['tango_ori'])\n\n    # Compute the IMU orientation in the Tango coordinate frame.\n    init_tango_ori = quaternion.quaternion(*tango_ori[0])\n    ori = rotation_vector\n    ori_q = quaternion.from_float_array(ori)\n    init_rotor = init_tango_ori * ori_q[0].conj()\n    ori_q = init_rotor * ori_q\n\n    gyro_q = quaternion.from_float_array(np.concatenate([np.zeros([gyro.shape[0], 1]), gyro], axis=1))\n    acce_q = quaternion.from_float_array(np.concatenate([np.zeros([acce.shape[0], 1]), acce], axis=1))\n    glob_gyro = quaternion.as_float_array(ori_q * gyro_q * ori_q.conj())[:, 1:]\n    glob_acce = quaternion.as_float_array(ori_q * acce_q * ori_q.conj())[:, 1:]\n\n    rawdata = np.concatenate((timestamps, glob_gyro, glob_acce), axis = 1)\n    groundtruth = np.concatenate((timestamps, tango_pos, tango_ori), axis = 1)\n\n    return rawdata, groundtruth\n\ndef convert_data_test(data_path):\n    \"\"\"\n    Data Processing\n    :param data_path\n    This function is used to convert the raw data\n    stored in hdf5 format to numpy array\n    \"\"\"\n    # read hdf5 file\n    file = h5py.File(os.path.join(data_path, 'data.hdf5'), 'r')\n    synced = file['synced'] # synced data (gyro, acce, magn ...)\n    pose = file['pose'] # pose data (ground truth)\n    # timestamp (1D)\n    timestamps = np.array(synced['time'])\n    timestamps = timestamps.reshape((len(timestamps), 1)) / 10**9 # - timestamps[0] # start from 0\n    timestamps = timestamps - timestamps[0]\n    # gyroscope (3D), accelerometer (3D), magnetometer (3D), rotation vector (4D)\n    gyro = np.array(synced['gyro'])\n    acce = np.array(synced['acce'])\n    # game rotation vector can convert acce and gyro\n    # from body frame to navigation frame\n    rotation_vector = synced['game_rv']\n\n    # position (3D), orientation (4D)\n    pose = file['pose']\n    tango_ori = np.array(pose['tango_ori'])\n\n    # Compute the IMU orientation in the Tango coordinate frame.\n    init_tango_ori = quaternion.quaternion(*tango_ori[0])\n    ori = rotation_vector\n    ori_q = quaternion.from_float_array(ori)\n    init_rotor = init_tango_ori * ori_q[0].conj()\n    ori_q = init_rotor * ori_q\n\n    gyro_q = quaternion.from_float_array(np.concatenate([np.zeros([gyro.shape[0], 1]), gyro], axis=1))\n    acce_q = quaternion.from_float_array(np.concatenate([np.zeros([acce.shape[0], 1]), acce], axis=1))\n    glob_gyro = quaternion.as_float_array(ori_q * gyro_q * ori_q.conj())[:, 1:]\n    glob_acce = quaternion.as_float_array(ori_q * acce_q * ori_q.conj())[:, 1:]\n\n    rawdata = np.concatenate((timestamps, glob_gyro, glob_acce), axis = 1)\n\n    return rawdata\n\nclass GlobSequence():\n    \"\"\"\n    Property: global coordinate frame\n    \"\"\"\n    # add 3-axis magnetometer\n    # feature_dim = 9\n    feature_dim = 6\n    target_dim = 2\n    aux_dim = 8\n\n    def __init__(self, data_path = None, **kwargs):\n        super().__init__()\n        self.ts, self.features, self.targets, self.gt_pos = None, None, None, None\n        # self.info = {}\n        self.w = kwargs.get('interval', 1)\n        if data_path is not None:\n            self.load(data_path)\n\n    def load(self, data_path):\n        # print(\"the data_path is:\", data_path)\n        data, ground_truth = convert_data(data_path)\n        # already in global coordinate frame and start from start frame\n        # timestamp (1D) gyroscope (3D), accelerometer (3D)\n        gyro = data[:, 1:4]\n        acce = data[:, 4:7]\n        ts = data[:, 0]\n        # tango position\n        tango_pos = ground_truth[:, 1:4]\n        # tango orientation\n        tango_ori = ground_truth[:, 4:8]\n\n        dt = (ts[self.w:] - ts[:-self.w])[:, None]\n        # calculate the global velocity\n        glob_v = (tango_pos[self.w:] - tango_pos[:-self.w]) / dt\n\n        self.ts = ts\n        self.features = np.concatenate([gyro, acce], axis = 1)\n        # We only use the global velocity in the floor plane\n        self.targets = glob_v[:, :2]\n        self.orientations = tango_ori # quaternion.as_float_array(tango_ori)\n        self.gt_pos = tango_pos\n\n    def get_feature(self):\n        return self.features\n\n    def get_target(self):\n        return self.targets\n\n    def get_aux(self):\n        return np.concatenate([self.ts[:, None], self.orientations, self.gt_pos], axis = 1)\n\nclass GlobSequenceTest():\n    \"\"\"\n    Property: global coordinate frame\n    \"\"\"\n    # add 3-axis magnetometer\n    # feature_dim = 9\n    feature_dim = 6\n    aux_dim = 8\n\n    def __init__(self, data_path = None, **kwargs):\n        super().__init__()\n        self.ts, self.features, self.targets, self.gt_pos = None, None, None, None\n        # self.info = {}\n        self.w = kwargs.get('interval', 1)\n        if data_path is not None:\n            self.load(data_path)\n\n    def load(self, data_path):\n        # print(\"the data_path is:\", data_path)\n        data = convert_data_test(data_path)\n        # already in global coordinate frame and start from start frame\n        # timestamp (1D) gyroscope (3D), accelerometer (3D)\n        gyro = data[:, 1:4]\n        acce = data[:, 4:7]\n        ts = data[:, 0]\n\n        dt = (ts[self.w:] - ts[:-self.w])[:, None]\n\n        self.ts = ts\n        self.features = np.concatenate([gyro, acce], axis = 1)\n\n    def get_feature(self):\n        return self.features\n\n    def get_aux(self):\n        return 0\n\ndef load_sequences(seq_type, root_dir, data_list, **kwargs):\n    features_all, targets_all, aux_all = [], [], []\n\n    for i in range(len(data_list)):\n        seq = seq_type(osp.join(root_dir, data_list[i]), **kwargs)\n        feat, targ, aux = seq.get_feature(), seq.get_target(), seq.get_aux()\n        # add feat, targ, aux to list\n        features_all.append(feat)\n        targets_all.append(targ)\n        aux_all.append(aux)\n    return features_all, targets_all, aux_all\n\ndef load_sequences_test(seq_type, root_dir, data_list, **kwargs):\n    features_all, aux_all = [], []\n\n    for i in range(len(data_list)):\n        seq = seq_type(osp.join(root_dir, data_list[i]), **kwargs)\n        feat, aux = seq.get_feature(), seq.get_aux()\n        # add feat, targ, aux to list\n        features_all.append(feat)\n        aux_all.append(aux)\n    return features_all, aux_all\n\nclass SequenceToSequenceDataset(Dataset):\n    def __init__(self, seq_type, root_dir, data_list, step_size = 100, window_size = 400,\n                 random_shift = 0, transform = None, **kwargs):\n        super(SequenceToSequenceDataset, self).__init__()\n        self.seq_type = seq_type\n        self.feature_dim = seq_type.feature_dim\n        self.target_dim = seq_type.target_dim\n        self.aux_dim = seq_type.aux_dim\n        self.window_size = window_size\n        self.step_size = step_size\n        self.random_shift = random_shift\n        self.transform = transform\n        self.projection_width = kwargs.get('projection_width', 0)\n\n        self.data_path = [osp.join(root_dir, data) for data in data_list]\n        self.index_map = []\n\n        self.features, self.targets, aux = load_sequences(\n            seq_type, root_dir, data_list, **kwargs)\n\n        # Optionally smooth the sequence\n        feat_sigma = kwargs.get('feature_sigma,', -1)\n        targ_sigma = kwargs.get('target_sigma,', -1)\n        if feat_sigma > 0:\n            self.features = [gaussian_filter1d(feat, sigma=feat_sigma, axis=0) for feat in self.features]\n        if targ_sigma > 0:\n            self.targets = [gaussian_filter1d(targ, sigma=targ_sigma, axis=0) for targ in self.targets]\n\n        max_norm = 3.0 #\n        self.ts, self.orientations, self.gt_pos, self.local_v = [], [], [], []\n        for i in range(len(data_list)):\n            self.features[i] = self.features[i][:-1]\n            self.targets[i] = self.targets[i]\n            self.ts.append(aux[i][:-1, :1])\n            self.orientations.append(aux[i][:-1, 1:5])\n            self.gt_pos.append(aux[i][:-1, 5:8])\n\n            velocity = np.linalg.norm(self.targets[i], axis=1)  # Remove outlier ground truth data\n            bad_data = velocity > max_norm\n            for j in range(window_size + random_shift, self.targets[i].shape[0], step_size):\n                if not bad_data[j - window_size - random_shift:j + random_shift].any():\n                    self.index_map.append([i, j])\n\n        # if shuffle is necessary here? As the training data should be in a logical sequence\n        if kwargs.get('shuffle', True):\n            random.shuffle(self.index_map)\n\n    def __getitem__(self, item):\n        # output format: input, target, seq_id, frame_id\n        seq_id, frame_id = self.index_map[item][0], self.index_map[item][1]\n\n        feat = np.copy(self.features[seq_id][frame_id - self.window_size:frame_id])\n        targ = np.copy(self.targets[seq_id][frame_id - self.window_size:frame_id])\n        # random rotate the sequence in the horizontal plane\n        if self.transform is not None:\n            feat, targ = self.transform(feat, targ)\n\n            return feat.astype(np.float32), targ.astype(np.float32), seq_id, frame_id\n\n    def __len__(self):\n        return len(self.index_map)\n\n    def get_lstm_test_seq(self):\n        return np.array(self.features).astype(np.float32), np.array(self.targets).astype(np.float32)\n\nclass SequenceToSequenceDatasetTest(Dataset):\n    def __init__(self, seq_type, root_dir, data_list, step_size = 100, window_size = 400,\n                 random_shift = 0, transform = None, **kwargs):\n        super(SequenceToSequenceDatasetTest, self).__init__()\n        self.seq_type = seq_type\n        self.feature_dim = seq_type.feature_dim\n        self.aux_dim = seq_type.aux_dim\n        self.window_size = window_size\n        self.step_size = step_size\n        self.random_shift = random_shift\n        self.transform = transform\n        self.projection_width = kwargs.get('projection_width', 0)\n\n        self.data_path = [osp.join(root_dir, data) for data in data_list]\n        self.index_map = []\n\n        self.features, aux = load_sequences_test(\n            seq_type, root_dir, data_list, **kwargs)\n\n        # Optionally smooth the sequence\n        feat_sigma = kwargs.get('feature_sigma,', -1)\n        if feat_sigma > 0:\n            self.features = [gaussian_filter1d(feat, sigma=feat_sigma, axis=0) for feat in self.features]\n\n        max_norm = 3.0 #\n        for i in range(len(data_list)):\n            self.features[i] = self.features[i][:-1]\n\n        # if shuffle is necessary here? As the training data should be in a logical sequence\n        if kwargs.get('shuffle', True):\n            random.shuffle(self.index_map)\n\n    def __getitem__(self, item):\n        # output format: input, target, seq_id, frame_id\n        seq_id, frame_id = self.index_map[item][0], self.index_map[item][1]\n        feat = np.copy(self.features[seq_id][frame_id - self.window_size:frame_id])\n\n        return feat.astype(np.float32), seq_id, frame_id\n\n    def __len__(self):\n        return len(self.index_map)\n\n    def get_lstm_test_seq(self):\n        return np.array(self.features).astype(np.float32)\n\ndef change_cf(ori, vectors):\n    \"\"\"\n    Euler-Rodrigous formula v'=v+2s(rxv)+2rx(rxv)\n    :param ori: quaternion [n]x4\n    :param vectors: vector nx3\n    :return: rotated vector nx3\n    \"\"\"\n    assert ori.shape[-1] == 4\n    assert vectors.shape[-1] == 3\n\n    if len(ori.shape) == 1:\n        ori = ori.reshape(1, -1)\n\n    q_s = ori[:, :1]\n    q_r = ori[:, 1:]\n\n    tmp = np.cross(q_r, vectors)\n    vectors = np.add(np.add(vectors, np.multiply(2, np.multiply(q_s, tmp))), np.multiply(2, np.cross(q_r, tmp)))\n    return vectors\n\nclass ComposeTransform:\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, feat, targ, **kwargs):\n        for t in self.transforms:\n            feat, targ = t(feat, targ)\n        return feat, targ\n\nclass RandomHoriRotateSeq:\n    def __init__(self, input_format, output_format=None):\n        \"\"\"\n        Rotate global input, global output by a random angle\n        @:param input format - input feature vector(x,3) boundaries as array (E.g [0,3,6])\n        @:param output format - output feature vector(x,2/3) boundaries as array (E.g [0,2,5])\n                                if 2, 0 is appended as z.\n        \"\"\"\n        self.i_f = input_format\n        self.o_f = output_format\n\n    def __call__(self, feature, target):\n        a = np.random.random() * 2 * np.math.pi\n        # print(\"Rotating by {} degrees\", a/np.math.pi * 180)\n        t = np.array([np.cos(a), 0, 0, np.sin(a)])\n\n        for i in range(len(self.i_f) - 1):\n            feature[:, self.i_f[i]: self.i_f[i + 1]] = \\\n                change_cf(t, feature[:, self.i_f[i]: self.i_f[i + 1]])\n\n        for i in range(len(self.o_f) - 1):\n            if self.o_f[i + 1] - self.o_f[i] == 3:\n                # vector = target[:, self.o_f[i]: self.o_f[i + 1]]\n                # target[:, self.o_f[i]: self.o_f[i + 1]] = change_cf(t, vector)\n                vector = target[self.o_f[i]: self.o_f[i + 1]]\n                target[:, self.o_f[i]: self.o_f[i + 1]] = change_cf(t, vector)\n            elif self.o_f[i + 1] - self.o_f[i] == 2:\n                vector = np.concatenate([target[:, self.o_f[i]: self.o_f[i + 1]], np.zeros([target.shape[0], 1])], axis=1)\n                target[:, self.o_f[i]: self.o_f[i + 1]] = change_cf(t, vector)[:, :2]\n\n        return feature.astype(np.float32), target.astype(np.float32)\n\nclass RandomHoriRotateSeqTensor:\n    def __init__(self):\n        \"\"\"\n        Rotate global input, global output by a random angle\n        @:param input format - input feature vector(x,3) boundaries as array (E.g [0,3,6])\n        @:param output format - output feature vector(x,2/3) boundaries as array (E.g [0,2,5])\n                                if 2, 0 is appended as z.\n        \"\"\"\n\n    def __call__(self, feature, target):\n        # Tensor random rotation matrix\n        a = torch.rand(1) * 2 * np.math.pi\n        rotation_matrix_feat = torch.tensor([[torch.cos(a), torch.sin(a), 0, 0, 0, 0],\n                                            [-torch.sin(a), torch.cos(a), 0, 0, 0, 0],\n                                            [0, 0, 1, 0, 0, 0],\n                                            [0, 0, 0, torch.cos(a), torch.sin(a), 0],\n                                            [0, 0, 0, -torch.sin(a), torch.cos(a), 0],\n                                            [0, 0, 0, 0, 0, 1]], dtype=torch.float32)\n\n        rotation_matrix_targ = torch.tensor([[torch.cos(a), torch.sin(a)],\n                                            [-torch.sin(a), torch.cos(a)]], dtype=torch.float32)\n\n        # Matrix multiplication\n        feature = torch.matmul(feature, rotation_matrix_feat)\n        target = torch.matmul(target, rotation_matrix_targ)\n\n        return feature, target\n\ndef get_dataset(root_dir, data_list, mode, **kwargs):\n    # load config\n    global_step_size = 0\n    # input data includes: accelemeters, gyroscopes\n    input_format = [0, 3, 6]\n    # output data is the moving distance and its direction\n    output_format = [0, 2]\n\n    random_shift, shuffle, transforms = 0, False, []\n\n    if mode == 'train':\n        random_shift = global_step_size // 2\n        shuffle = True\n        transforms.append(RandomHoriRotateSeq(input_format, output_format))\n        global_step_size = kwargs.get('step_size')\n        transforms = ComposeTransform(transforms)\n        seq_type = GlobSequence\n        global_window_size = kwargs.get('window_size')\n        dataset = SequenceToSequenceDataset(seq_type, root_dir, data_list, global_step_size,\n                                            global_window_size, random_shift = random_shift,\n                                            transform = transforms, shuffle = shuffle)\n    elif mode == 'val':\n        shuffle = True\n        global_step_size = kwargs.get('step_size')\n        transforms = ComposeTransform(transforms)\n        seq_type = GlobSequence\n        global_window_size = kwargs.get('window_size')\n        dataset = SequenceToSequenceDataset(seq_type, root_dir, data_list, global_step_size,\n                                            global_window_size, random_shift = random_shift,\n                                            transform = transforms, shuffle = shuffle)\n    elif mode == 'val_test':\n        shuffle = False\n        global_step_size = kwargs.get('test_step_size')\n        transforms = ComposeTransform(transforms)\n        seq_type = GlobSequence\n        global_window_size = kwargs.get('window_size')\n        dataset = SequenceToSequenceDataset(seq_type, root_dir, data_list, global_step_size,\n                                            global_window_size, random_shift = random_shift,\n                                            transform = transforms, shuffle = shuffle)\n    elif mode == 'test':\n        shuffle = False\n        global_step_size = kwargs.get('test_step_size')\n        transforms = ComposeTransform(transforms)\n        seq_type = GlobSequenceTest\n        global_window_size = kwargs.get('window_size')\n        dataset = SequenceToSequenceDatasetTest(seq_type, root_dir, data_list, global_step_size,\n                                                global_window_size, random_shift = random_shift,\n                                                transform = transforms, shuffle = shuffle)\n\n    return dataset\n\ndef read_dir(dir_path):\n    # read dirs from dir_path\n    for _, dirs, _ in os.walk(dir_path):\n        return dirs\n\ndef get_train_dataset(root_dir, **kwargs):\n    trainlist = read_dir(root_dir)\n    return get_dataset(root_dir, trainlist, mode = 'train', **kwargs)\n\ndef get_valid_dataset(root_dir, **kwargs):\n    validlist = read_dir(root_dir)\n    return get_dataset(root_dir, validlist, mode = 'val', **kwargs)\n\ndef get_valid_test_dataset(root_dir, dir, **kwargs):\n    return get_dataset(root_dir, dir, mode = 'val_test', **kwargs)\n\ndef get_test_dataset(root_dir, dir, **kwargs):\n    return get_dataset(root_dir, dir, mode = 'test', **kwargs)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T11:43:11.443551Z","iopub.execute_input":"2023-10-10T11:43:11.443903Z","iopub.status.idle":"2023-10-10T11:43:13.257058Z","shell.execute_reply.started":"2023-10-10T11:43:11.443873Z","shell.execute_reply":"2023-10-10T11:43:13.256102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Criterion","metadata":{}},{"cell_type":"code","source":"import json\nimport os\nimport sys\nimport time\nimport random\nimport argparse\nfrom os import path as osp\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\n\nclass GlobalPosLoss(torch.nn.Module):\n    def __init__(self):\n        \"\"\"\n        Calculate position loss in global coordinate frame\n        Target :- Global Velocity\n        Prediction :- Global Velocity\n        \"\"\"\n        super(GlobalPosLoss, self).__init__()\n        self.mse_loss = torch.nn.MSELoss(reduction = 'none')\n\n    def forward(self, pred, targ):\n        # dts = 1 / 200\n        dts = 1\n        pred = pred * dts\n        targ = targ * dts\n        gt_pos = torch.cumsum(targ[:, 1:, ], 1)\n        pred_pos = torch.cumsum(pred[:, 1:, ], 1)\n        loss = self.mse_loss(pred_pos, gt_pos)\n        # calculate the sum of absolute trajectory error\n        return torch.mean(loss)\n\nclass MSEAverage():\n    def __init__(self):\n        self.count = 0\n        self.targets = []\n        self.predictions = []\n        self.average = []\n\n    def add(self, pred, targ):\n        self.targets.append(targ)\n        self.predictions.append(pred)\n        self.average.append(np.average((pred - targ) ** 2, axis=(0, 1)))\n        # print(\"The shape of average is: \", np.array(self.average).shape)\n        # print(\"THe shape of np.average(np.array(self.average), axis=0) is: \", np.average(np.array(self.average), axis=0).shape)\n        self.count += 1\n\n    def get_channel_avg(self):\n        average = np.average(np.array(self.average), axis=0)\n        return average\n\n    def get_total_avg(self):\n        average = np.average(np.array(self.average), axis=0)\n        return np.average(average)\n\n    def get_elements(self, axis):\n        return np.concatenate(self.predictions, axis=axis), np.concatenate(self.targets, axis=axis)\n\ndef reconstruct_traj(vector, **kwargs):\n    global_sampling_rate = kwargs.get('sampling_rate', None)\n    # reconstruct the vector to one sequence\n    # velocity_sequence = vector.reshape(len(vector) * global_window_size, global_output_channel)\n\n    velocity_sequence = vector * 1 / global_sampling_rate\n    glob_pos = np.cumsum(velocity_sequence, axis = 0)\n\n    return glob_pos\n\ndef compute_absolute_trajectory_error(pred, gt):\n    \"\"\"\n    The Absolute Trajectory Error (ATE) defined in:\n    A Benchmark for the evaluation of RGB-D SLAM Systems\n    http://ais.informatik.uni-freiburg.de/publications/papers/sturm12iros.pdf\n\n    Args:\n        est: estimated trajectory\n        gt: ground truth trajectory. It must have the same shape as est.\n\n    Return:\n        Absolution trajectory error, which is the Root Mean Squared Error between\n        two trajectories.\n    \"\"\"\n    return np.sqrt(np.mean((pred - gt) ** 2))\n\n\ndef compute_relative_trajectory_error(est, gt, delta, max_delta = -1):\n    \"\"\"\n    The Relative Trajectory Error (RTE) defined in:\n    A Benchmark for the evaluation of RGB-D SLAM Systems\n    http://ais.informatik.uni-freiburg.de/publications/papers/sturm12iros.pdf\n\n    Args:\n        est: the estimated trajectory\n        gt: the ground truth trajectory.\n        delta: fixed window size. If set to -1, the average of all RTE up to max_delta will be computed.\n        max_delta: maximum delta. If -1 is provided, it will be set to the length of trajectories.\n\n    Returns:\n        Relative trajectory error. This is the mean value under different delta.\n    \"\"\"\n    if max_delta == -1:\n        max_delta = est.shape[0]\n    # print(\"delta: \", delta)\n    deltas = np.array([min(delta, max_delta - 1)])\n    # deltas = np.array([delta]) if delta > 0 else np.arange(1, min(est.shape[0], max_delta))\n    rtes = np.zeros(deltas.shape[0])\n    for i in range(deltas.shape[0]):\n        # For each delta, the RTE is computed as the RMSE of endpoint drifts from fixed windows\n        # slided through the trajectory.\n        err = est[deltas[i]:] + gt[:-deltas[i]] - est[:-deltas[i]] - gt[deltas[i]:]\n        rtes[i] = np.sqrt(np.mean(err ** 2))\n\n    # The average of RTE of all window sized is returned.\n    rtes = rtes[~np.isnan(rtes)]\n    return np.mean(rtes)\n\ndef compute_position_drift_error(pos_pred, pos_gt):\n    \"\"\"\n    Params:\n        pos_pred: predicted position [seq_len, 2]\n        pos_gt: ground truth position [seq_len, 2]\n    \"\"\"\n    position_drift = np.linalg.norm((pos_gt[-1] - pos_pred[-1]))\n    delta_position = pos_gt[1:] - pos_gt[:-1]\n    delta_length = np.linalg.norm(delta_position, axis=1)\n    moving_len = np.sum(delta_length)\n\n    return position_drift / moving_len\n\ndef compute_distance_error(pos_pred, pos_gt):\n    \"\"\"\n    Params:\n        pos_pred: predicted position [seq_len, 2]\n        pos_gt: ground truth position [seq_len, 2]\n    \"\"\"\n    distance_error = np.linalg.norm((pos_gt - pos_pred), axis=1)\n\n    return distance_error\n\ndef compute_heading_error(preds, targets):\n    \"\"\"\n    Params:\n        pos_pred: predicted position [seq_len, 2]\n        pos_gt: ground truth position [seq_len, 2]\n    \"\"\"\n    # Find the index of preds with zero norm\n    zero_norm_index = np.where(np.linalg.norm(preds, axis=1) == 0)[0]\n    # Remove the zero norm index\n    preds = np.delete(preds, zero_norm_index, axis=0)\n    targets = np.delete(targets, zero_norm_index, axis=0)\n    # Find the index of targets with zero norm\n    zero_norm_index = np.where(np.linalg.norm(targets, axis=1) == 0)[0]\n    # Remove the zero norm index\n    preds = np.delete(preds, zero_norm_index, axis=0)\n    targets = np.delete(targets, zero_norm_index, axis=0)\n\n    pred_v = np.linalg.norm(preds, axis=1)\n    targ_v = np.linalg.norm(targets, axis=1)\n\n    pred_o = preds / pred_v[:, np.newaxis]\n    targ_o = targets / targ_v[:, np.newaxis]\n\n    # calculate the heading angle of the predicted and target vectors\n    pred_heading = np.arctan2(pred_o[:, 1], pred_o[:, 0])\n    targ_heading = np.arctan2(targ_o[:, 1], targ_o[:, 0])\n\n    # calculate the heading error\n    heading_error = np.mean(np.abs(pred_heading - targ_heading))\n    # convert to degrees\n    heading_error = heading_error * 180 / np.pi\n\n    return heading_error","metadata":{"execution":{"iopub.status.busy":"2023-10-10T11:43:13.258691Z","iopub.execute_input":"2023-10-10T11:43:13.259205Z","iopub.status.idle":"2023-10-10T11:43:13.278108Z","shell.execute_reply.started":"2023-10-10T11:43:13.259174Z","shell.execute_reply":"2023-10-10T11:43:13.277334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Utils","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os.path as osp\n\ndef format_string(*argv, sep=' '):\n    result = ''\n    for val in argv:\n        if isinstance(val, (tuple, list, np.ndarray)):\n            for v in val:\n                result += format_string(v, sep=sep) + sep\n        else:\n            result += str(val) + sep\n    return result[:-1]\n\ndef draw_trajectory(pos_pred, pos_gt, dir_name, ate, rte, **kwargs):\n    \"\"\"\n    :param data:\n    :pos_pred: (N, 2)\n    :pos_gt: (N, 2)\n    :dir_name: test directory\n    :ate: average trajectory error\n    :rte: relative trajectory error\n    \"\"\"\n    global_out_dir = kwargs.get('out_dir', None)\n    if global_out_dir is None:\n        raise ValueError('out_dir is needed')\n\n    plt.figure(figsize=(8, 5), dpi = 400)\n    plt.plot(pos_pred[:, 0], pos_pred[:, 1], label = 'Predicted')\n    plt.plot(pos_gt[:, 0], pos_gt[:, 1], label = 'Ground truth')\n    plt.title(dir_name)\n    print(\"make title success\")\n    # Show words in latex format\n    plt.xlabel('$m$')\n    plt.ylabel('$m$')\n    plt.axis('equal')\n    plt.legend()\n    plt.title('ATE:{:.3f}, RTE:{:.3f}'.format(ate, rte), y = 0, loc = 'right')\n\n    plt.savefig(osp.join(global_out_dir, '{}.png'.format(dir_name)))\n","metadata":{"execution":{"iopub.status.busy":"2023-10-10T11:43:15.031406Z","iopub.execute_input":"2023-10-10T11:43:15.031757Z","iopub.status.idle":"2023-10-10T11:43:15.039392Z","shell.execute_reply.started":"2023-10-10T11:43:15.031730Z","shell.execute_reply":"2023-10-10T11:43:15.038501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6. Main Function","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport argparse\nfrom os import path as osp\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport numpy as np\nimport torch\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader\n\ndef get_model(mode, **kwargs):\n    global_input_channel = kwargs.get('input_channel')\n    global_output_channel = kwargs.get('output_channel')\n    global_dropout = kwargs.get('dropout')\n    global_batch_size = kwargs.get('batch_size')\n    global_test_batch_size = kwargs.get('test_batch_size')\n    global_device = kwargs.get('device')\n    global_layers = kwargs.get('layers')\n    global_layer_size = kwargs.get('layer_size')\n\n    if mode == 'train':\n        print(\"LSTM model\")\n        network = BilinearLSTMSeqNetwork(global_input_channel, global_output_channel, global_batch_size, global_device,\n                        lstm_size = global_layer_size, lstm_layers = global_layers, dropout = global_dropout).to(global_device)\n    elif mode == 'test':\n        print(\"LSTM model\")\n        network = BilinearLSTMSeqNetwork(global_input_channel, global_output_channel, global_test_batch_size, global_device,\n                        lstm_size = global_layer_size, lstm_layers = global_layers, dropout = global_dropout).to(global_device)\n    try:\n        pytorch_total_params = sum(p.numel() for p in network.parameters() if p.requires_grad)\n    except:\n        pytorch_total_params = 0\n    print('Network constructed. trainable parameters: {}'.format(pytorch_total_params))\n    return network\n\ndef train(**kwargs):\n    # load config\n    global_data_dir = kwargs.get('data_dir')\n    global_val_data_dir = kwargs.get('val_data_dir')\n    global_batch_size = kwargs.get('batch_size')\n    global_epochs = kwargs.get('epochs')\n    global_num_workers = kwargs.get('num_workers')\n    global_device = kwargs.get('device')\n    global_out_dir = kwargs.get('out_dir', None)\n    global_learning_rate = kwargs.get('learning_rate')\n    global_save_interval = kwargs.get('save_interval')\n    global_sampling_rate = kwargs.get('sampling_rate')\n    # Loading data\n    start_t = time.time()\n    train_dataset = get_train_dataset(global_data_dir, **kwargs)\n    val_dataset = get_valid_dataset(global_val_data_dir, **kwargs)\n    train_loader = DataLoader(train_dataset, batch_size = global_batch_size, num_workers = global_num_workers, shuffle = True,\n                              drop_last = True)\n    val_loader = DataLoader(val_dataset, batch_size = global_batch_size, shuffle = True, drop_last = True)\n    end_t = time.time()\n    print('Training and validation set loaded. Time usage: {:.3f}s'.format(end_t - start_t))\n    # read val for sequence test\n    test_dirs = read_dir(global_val_data_dir)\n\n\n\n    global device\n    device = torch.device(global_device if torch.cuda.is_available() else 'cpu')\n    print(\"Device: {}\".format(device))\n\n    if global_out_dir:\n        if not osp.isdir(global_out_dir):\n            os.makedirs(global_out_dir)\n        if not osp.isdir(osp.join(global_out_dir, 'checkpoints')):\n            os.makedirs(osp.join(global_out_dir, 'checkpoints'))\n\n    print('\\nNumber of train samples: {}'.format(len(train_dataset)))\n    train_mini_batches = len(train_loader)\n    if val_dataset:\n        print('Number of val samples: {}'.format(len(val_dataset)))\n        val_mini_batches = len(val_loader)\n\n    network = get_model('train', **kwargs).to(device)\n    testnetwork = get_model('test', **kwargs).to(device)\n    criterion = GlobalPosLoss()\n\n    optimizer = torch.optim.Adam(network.parameters(), global_learning_rate)\n    scheduler = ReduceLROnPlateau(optimizer, 'min', patience = 10, factor = 0.75, verbose = True, eps = 1e-12)\n    quiet_mode = kwargs.get('quiet', False)\n    use_scheduler = kwargs.get('use_scheduler', True)\n\n    start_epoch = 0\n    step = 0\n    best_val_loss = np.inf\n    train_errs = np.zeros(global_epochs)\n\n    print(\"Starting from epoch {}\".format(start_epoch))\n    try:\n        for epoch in range(start_epoch, global_epochs):\n            log_line = ''\n            network.train()\n            train_vel = MSEAverage()\n            train_loss = 0\n            start_t = time.time()\n\n            for bid, batch in enumerate(tqdm(train_loader)):\n                feat, targ, _, _ = batch\n                feat, targ = feat.to(device), targ.to(device)\n                optimizer.zero_grad()\n                predicted = network(feat)\n                train_vel.add(predicted.cpu().detach().numpy(), targ.cpu().detach().numpy())\n                loss = criterion(predicted, targ)\n                train_loss += loss.cpu().detach().numpy()\n                loss.backward()\n                optimizer.step()\n                step += 1\n\n            train_errs[epoch] = train_loss / train_mini_batches\n            end_t = time.time()\n            if not quiet_mode:\n                print('-' * 25)\n                print('Epoch {}, time usage: {:.3f}s, loss: {}, vec_loss {}/{:.6f}'.format(\n                    epoch, end_t - start_t, train_errs[epoch], train_vel.get_channel_avg(), train_vel.get_total_avg()))\n\n            saved_model = False\n            if val_loader:\n                network.eval()\n                val_vel = MSEAverage()\n                val_loss = 0\n                for bid, batch in enumerate(val_loader):\n                    feat, targ, _, _ = batch\n                    feat, targ = feat.to(device), targ.to(device)\n                    optimizer.zero_grad()\n                    pred = network(feat)\n                    val_vel.add(pred.cpu().detach().numpy(), targ.cpu().detach().numpy())\n                    val_loss += criterion(pred, targ).cpu().detach().numpy()\n                val_loss = val_loss / val_mini_batches\n\n                if not quiet_mode:\n                    print('Validation loss: {} vec_loss: {}/{:.6f}'.format(val_loss, val_vel.get_channel_avg(),\n                                                                                val_vel.get_total_avg()))\n\n                if val_loss < best_val_loss:\n                    best_val_loss = val_loss\n                    saved_model = True\n                    if global_out_dir:\n                        model_path = osp.join(global_out_dir, 'checkpoints', 'checkpoint_%d.pt' % epoch)\n                        torch.save({'model_state_dict': network.state_dict(),\n                                    'epoch': epoch,\n                                    'loss': train_errs[epoch],\n                                    'optimizer_state_dict': optimizer.state_dict()}, model_path)\n                        print('Best Validation Model saved to ' + model_path)\n                if use_scheduler:\n                    scheduler.step(val_loss)\n\n            print(\"Reconstruct the validation trajectory and compute the ATE, RTE, PDE, and AYE\")\n            testnetwork.load_state_dict(network.state_dict())\n            network.eval().to(device)\n            ate_all, rte_all, pde_all = [], [], []\n            aye_all = []\n            # Every minute\n            pred_per_min = global_sampling_rate * 60 # 2 + 0.5*(x - 1) = 60\n            # Test for every sequence\n            for i in range(len(test_dirs)):\n            # for i in range(1):\n                seq_dir = [test_dirs[i]]\n                seq_dataset = get_valid_test_dataset(global_val_data_dir, seq_dir, **kwargs)\n                feat, targ = seq_dataset.get_lstm_test_seq()\n\n                feat = torch.from_numpy(feat).to(device)\n                pred = testnetwork(feat).cpu().detach().numpy()\n\n                pred = np.squeeze(pred, axis = 0)\n                targ = np.squeeze(targ, axis = 0)\n                # Reconstruct the trajectory\n                pos_pred = reconstruct_traj(pred, **kwargs)\n                pos_gt = reconstruct_traj(targ, **kwargs)\n\n                # Compute the ATE and RTE\n                ate = compute_absolute_trajectory_error(pos_pred, pos_gt)\n                rte = compute_relative_trajectory_error(pos_pred, pos_gt, delta = pred_per_min)\n                pde = compute_position_drift_error(pos_pred, pos_gt)\n                heading_error = compute_heading_error(pred, targ)\n                ate_all.append(ate)\n                if rte >= 0:\n                    rte_all.append(rte)\n                pde_all.append(pde)\n                aye_all.append(heading_error)\n\n            ate_all = np.array(ate_all)\n            rte_all = np.array(rte_all)\n            pde_all = np.array(pde_all)\n            aye_all = np.array(aye_all)\n\n            measure = format_string('ATE', 'RTE', 'PDE', 'AYE',  sep = '\\t')\n            values = format_string(np.mean(ate_all), np.mean(rte_all), np.mean(pde_all), np.mean(aye_all), sep = '\\t')\n            print(measure + '\\n' + values)\n\n            if global_out_dir and not saved_model and (epoch + 1) % global_save_interval == 0:  # save even with validation\n                model_path = osp.join(global_out_dir, 'checkpoints', 'icheckpoint_%d.pt' % epoch)\n                torch.save({'model_state_dict': network.state_dict(),\n                            'epoch': epoch,\n                            'loss': train_errs[epoch],\n                            'optimizer_state_dict': optimizer.state_dict()}, model_path)\n                print('Model saved to ' + model_path)\n\n            if np.isnan(train_loss):\n                print(\"Invalid value. Stopping training.\")\n                break\n    except KeyboardInterrupt:\n        print('-' * 60)\n        print('Early terminate')\n\n    print('Training completed')\n    if global_out_dir:\n        model_path = osp.join(global_out_dir, 'checkpoints', 'checkpoint_latest.pt')\n        torch.save({'model_state_dict': network.state_dict(),\n                    'epoch': epoch,\n                    'optimizer_state_dict': optimizer.state_dict()}, model_path)\n\n# Test and Save the Trajectory for Both Seen and Unseen Data\nseen_unseen_dataset = {'id1': 'tracermini_hw101_test20230311112635T',\n                'id2': 'tracermini_hw101_test20230311111507T',\n                'id3': 'tracermini_hw101_test20230311112235T',\n                'id4': 'tracermini_hw101_test20230313011357T',\n                'id5': 'tracermini_hw101_test20230311111842T',\n                'id6': 'tracermini_hw101_test20230313010954T',\n                'id7': 'tracermini_hw101_test20230313010546T',\n                'id8': 'tracermini_hw101_test20230313011731T',\n                'id9': 'tracermini_hw101_test20230313013335T',\n                'id10': 'tracermini_hw101_test20230311111027T',\n                'id11': 'tracermini_hw101_test20230313012956T',\n                'id12': 'tracermini_hw101_test20230313010204T',\n                'id13': 'tracermini_unseen_hw520230314091844T',\n                'id14': 'tracermini_unseen_cym20230314101001T',\n                'id15': 'tracermini_unseen_hw520230314082319T',\n                'id16': 'tracermini_unseen_cym20230314101636T',\n                'id17': 'tracermini_unseen_hw520230314083031T',\n                'id18': 'tracermini_unseen_hw520230314091110T',\n                'id19': 'tracermini_unseen_cym20230314100816T',\n                'id20': 'tracermini_unseen_cym20230314101230T',\n                'id21': 'tracermini_unseen_cym20230314100559T',\n                'id22': 'tracermini_unseen_hw520230314081212T',\n                'id23': 'tracermini_unseen_hw520230314082000T',\n                'id24': 'tracermini_unseen_hw520230314091603T',\n                'id25': 'tracermini_unseen_cym20230314100325T',\n                'id26': 'tracermini_unseen_cym20230314101434T',\n                'id27': 'tracermini_unseen_cym20230314100103T',\n                'id28': 'tracermini_unseen_hw520230314091338T',\n                'id29': 'tracermini_unseen_cym20230314101010T',\n                'id30': 'tracermini_unseen_cym20230314101850T',\n                'id31': 'tracermini_unseen_hw520230314081542T',\n                'id32': 'tracermini_unseen_hw520230314090739T'}\nseen_unseen_pred = {'id1': [],\n            'id2': [],\n            'id3': [],\n            'id4': [],\n            'id5': [],\n            'id6': [],\n            'id7': [],\n            'id8': [],\n            'id9': [],\n            'id10': [],\n            'id11': [],\n            'id12': [],\n            'id13': [],\n            'id14': [],\n            'id15': [],\n            'id16': [],\n            'id17': [],\n            'id18': [],\n            'id19': [],\n            'id20': [],\n            'id21': [],\n            'id22': [],\n            'id23': [],\n            'id24': [],\n            'id25': [],\n            'id26': [],\n            'id27': [],\n            'id28': [],\n            'id29': [],\n            'id30': [],\n            'id31': [],\n            'id32': []}\ndef test_lstm(**kwargs):\n    # load config\n    global_dataset = kwargs.get('dataset')\n    global_model_type = kwargs.get('model_type')\n    global_num_workers = kwargs.get('num_workers')\n    global_sampling_rate = kwargs.get('sampling_rate')\n    global_device = kwargs.get('device')\n    global_out_dir = kwargs.get('out_dir', None)\n    global_test_dir = kwargs.get('test_dir', None)\n    global_out_dir = kwargs.get('out_dir', None)\n    global_model_path = kwargs.get('model_path', None)\n\n    global device\n    device = torch.device(global_device if torch.cuda.is_available() else 'cpu')\n\n    if global_test_dir is None:\n        raise ValueError('Test_path is needed.')\n\n    # read dirs\n    test_dirs = read_dir(global_test_dir)\n\n    # Make sure the test output dir exists\n    if global_out_dir and not osp.exists(global_out_dir):\n        os.makedirs(global_out_dir)\n    # Load the model config\n    if global_model_path is None:\n        raise ValueError('Model path is needed.')\n\n    checkpoint = torch.load(global_model_path, map_location=global_device)\n\n    network = get_model('test', **kwargs)\n    network.load_state_dict(checkpoint.get('model_state_dict'))\n    # network.load_state_dict(checkpoint.get('model_state_dict'))\n    print(\"The model is loaded.\")\n    network.eval().to(device)\n    print('Model {} loaded to device {}.'.format(global_model_path, device))\n\n    # Test for every sequence\n    for i in range(len(test_dirs)):\n    # for i in range(1):\n        seq_dir = [test_dirs[i]]\n        seq_dataset = get_test_dataset(global_test_dir, seq_dir, **kwargs)\n        feat = seq_dataset.get_lstm_test_seq()\n\n        feat = torch.from_numpy(feat).to(device)\n        pred = network(feat).cpu().detach().numpy()\n\n        pred = np.squeeze(pred, axis = 0)\n\n        # Reconstruct the trajectory\n        print(\"Reconstruct the {}\".format(seq_dir[0]))\n        pos_pred = reconstruct_traj(pred, **kwargs)\n\n        # Find the index of the sequence\n        for key, value in seen_unseen_dataset.items():\n            if value == seq_dir[0]:\n                index = key\n                seen_unseen_pred[index] = pos_pred\n\n        # # Make directory\n        # if global_out_dir and not osp.exists(osp.join(global_out_dir, seq_dir[0])):\n        #     os.makedirs(osp.join(global_out_dir, seq_dir[0]))\n        # # Save the trajectory\n        # np.save(osp.join(global_out_dir, seq_dir[0], 'pred.npy'), pos_pred)\n\ndef test(**kwargs):\n    # Model type\n    print(\"Testing LSTM model\")\n    test_lstm(**kwargs)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T11:43:17.152100Z","iopub.execute_input":"2023-10-10T11:43:17.152881Z","iopub.status.idle":"2023-10-10T11:43:17.187872Z","shell.execute_reply.started":"2023-10-10T11:43:17.152847Z","shell.execute_reply":"2023-10-10T11:43:17.187008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7. Training","metadata":{}},{"cell_type":"code","source":"# Load config settings\nkwargs = load_config()\n\nimport warnings\n# Suspend warnings\nwarnings.filterwarnings('ignore')\ntrain(**kwargs)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T11:43:19.098234Z","iopub.execute_input":"2023-10-10T11:43:19.098928Z","iopub.status.idle":"2023-10-10T11:43:19.103422Z","shell.execute_reply.started":"2023-10-10T11:43:19.098899Z","shell.execute_reply":"2023-10-10T11:43:19.102358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check Saved Checkpoints\n# Define the directory path\ndir_path = '/kaggle/working/prediction_model/checkpoints'\n\n# Loop over all files in the directory\nfor filename in os.listdir(dir_path):\n    # Check if the file is a regular file (not a directory)\n    if os.path.isfile(os.path.join(dir_path, filename)):\n        # Do something with the file\n        print(filename)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T12:57:26.779225Z","iopub.execute_input":"2023-10-10T12:57:26.779618Z","iopub.status.idle":"2023-10-10T12:57:26.786913Z","shell.execute_reply.started":"2023-10-10T12:57:26.779592Z","shell.execute_reply":"2023-10-10T12:57:26.786031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Download the checkpoints.","metadata":{}},{"cell_type":"code","source":"!zip -r checkpoints.zip /kaggle/working/prediction_model","metadata":{"execution":{"iopub.status.busy":"2023-10-10T12:47:51.224730Z","iopub.execute_input":"2023-10-10T12:47:51.225081Z","iopub.status.idle":"2023-10-10T12:47:58.520725Z","shell.execute_reply.started":"2023-10-10T12:47:51.225053Z","shell.execute_reply":"2023-10-10T12:47:58.519550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'checkpoints.zip')","metadata":{"execution":{"iopub.status.busy":"2023-10-10T12:48:34.136079Z","iopub.execute_input":"2023-10-10T12:48:34.136453Z","iopub.status.idle":"2023-10-10T12:48:34.145241Z","shell.execute_reply.started":"2023-10-10T12:48:34.136421Z","shell.execute_reply":"2023-10-10T12:48:34.144215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 8. Testing","metadata":{}},{"cell_type":"markdown","source":"#### 8.1 Test for Seen Dataset","metadata":{}},{"cell_type":"code","source":"# Please change the output dir & model path\nTEST_DIR = '/kaggle/input/comp7310-project-1-imu-indoor-tracking/original_data/test_seen' # Dataset directory for testing (unseen_subjects_test_set)\nOUT_DIR = '/kaggle/working/test_results/test_seen' # Output directory for both traning and testing\nMODEL_PATH = '/kaggle/working/prediction_model/checkpoints/checkpoint_149.pt' # Model path for testing\n# Load config settings\nkwargs = load_config()\n\nimport warnings\n# Suspend warnings\nwarnings.filterwarnings('ignore')\ntest(**kwargs)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T12:49:50.035055Z","iopub.execute_input":"2023-10-10T12:49:50.035416Z","iopub.status.idle":"2023-10-10T12:49:50.040130Z","shell.execute_reply.started":"2023-10-10T12:49:50.035388Z","shell.execute_reply":"2023-10-10T12:49:50.038991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 8.2 Test for Unseen Dataset","metadata":{}},{"cell_type":"code","source":"# Please change the output dir & model path\nTEST_DIR = '/kaggle/input/comp7310-project-1-imu-indoor-tracking/original_data/test_unseen' # Dataset directory for testing (unseen_subjects_test_set)\nOUT_DIR = '/kaggle/working/test_results/test_unseen' # Output directory for both traning and testing\nMODEL_PATH = '/kaggle/working/prediction_model/checkpoints/checkpoint_149.pt' # Model path for testing\n# Load config settings\nkwargs = load_config()\nimport warnings\n# Suspend warnings\nwarnings.filterwarnings('ignore')\ntest(**kwargs)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T12:50:14.335907Z","iopub.execute_input":"2023-10-10T12:50:14.336590Z","iopub.status.idle":"2023-10-10T12:50:14.340970Z","shell.execute_reply.started":"2023-10-10T12:50:14.336557Z","shell.execute_reply":"2023-10-10T12:50:14.340031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 8.3 Check the Results","metadata":{}},{"cell_type":"code","source":"for key, value in seen_unseen_pred.items():\n    # print the length of each sequence\n    print(seen_unseen_dataset[key], len(value))","metadata":{"execution":{"iopub.status.busy":"2023-10-10T12:50:22.842965Z","iopub.execute_input":"2023-10-10T12:50:22.843344Z","iopub.status.idle":"2023-10-10T12:50:22.849114Z","shell.execute_reply.started":"2023-10-10T12:50:22.843311Z","shell.execute_reply":"2023-10-10T12:50:22.848223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 8.4 Save the Results","metadata":{}},{"cell_type":"code","source":"with open(\"submission.csv\", \"w\") as f:\n    # The first row must be \"Id, Category\"\n    f.write(\"Id,Prediction\\n\")\n\n    # For the rest of the rows, each image id corresponds to a predicted class.\n    for key, value in seen_unseen_pred.items():\n        # print the length of each sequence\n        print(seen_unseen_dataset[key], len(value))\n        # print the prediction\n        for i in range(len(value)):\n            f.write(\"{},{}\\n\".format(key+'_'+str(i), value[i]))","metadata":{"execution":{"iopub.status.busy":"2023-10-10T12:50:23.684518Z","iopub.execute_input":"2023-10-10T12:50:23.685195Z","iopub.status.idle":"2023-10-10T12:51:41.907052Z","shell.execute_reply.started":"2023-10-10T12:50:23.685164Z","shell.execute_reply":"2023-10-10T12:51:41.906019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}